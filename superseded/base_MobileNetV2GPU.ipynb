{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_Hdzn0RNOe3"
      },
      "source": [
        "# Face Emotion Recognition\n",
        "\n",
        "https://huggingface.co/datasets/tukey/human_face_emotions_roboflow/viewer/default/train?p=1&views%5B%5D=train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GImiTJQNN4w1"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KGB-_X0ENAHH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "df = pd.read_parquet(\"hf://datasets/tukey/human_face_emotions_roboflow/data/train-00000-of-00001.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS-C4FYGNyuN"
      },
      "source": [
        "# Data Overview & Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6BqKKM0N3XI",
        "outputId": "c84e2b99-5e69-4814-bb3e-61c456fbde77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values per column:\n",
            "image    0\n",
            "qa       0\n",
            "dtype: int64\n",
            "\n",
            "Dataframe Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9400 entries, 0 to 9399\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   image   9400 non-null   object\n",
            " 1   qa      9400 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 147.0+ KB\n",
            "None\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "                                               image  \\\n",
            "0  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
            "1  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
            "2  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
            "3  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
            "4  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
            "\n",
            "                                                  qa  \n",
            "0  [{'question': 'How does the person feel in the...  \n",
            "1  [{'question': 'How does the person feel in the...  \n",
            "2  [{'question': 'How does the person feel in the...  \n",
            "3  [{'question': 'How does the person feel in the...  \n",
            "4  [{'question': 'How does the person feel in the...  \n"
          ]
        }
      ],
      "source": [
        "# Standardize column names (strip whitespace, lower-case, replace spaces with underscores)\n",
        "df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isna().sum())\n",
        "\n",
        "# No missing values or duplicates, so we can proceed with the data as is\n",
        "\n",
        "# Print out summary information\n",
        "print(\"\\nDataframe Info:\")\n",
        "print(df.info())\n",
        "\n",
        "# Print the first few rows to inspect the data\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxRmSAs7TI6k",
        "outputId": "34b6a0d9-01e6-454f-b244-8446a5ff576c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing values per column:\n",
            "image    0\n",
            "qa       0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values in each column\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpsQexVtTM4d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9AqZbvTR0k9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5oYLwpbaR6io"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def extract_emotion(qa_entry):\n",
        "    try:\n",
        "        # If the qa_entry is a string, strip it and parse as JSON.\n",
        "        if isinstance(qa_entry, str):\n",
        "            qa_entry = qa_entry.strip()\n",
        "            qa_data = json.loads(qa_entry)\n",
        "        else:\n",
        "            qa_data = qa_entry\n",
        "\n",
        "        # If the data is a numpy array, convert it to a list.\n",
        "        if isinstance(qa_data, np.ndarray):\n",
        "            qa_data = qa_data.tolist()\n",
        "\n",
        "        # Now you can check if it's a list or tuple using this condition.\n",
        "        if isinstance(qa_data, (list, tuple)) and len(qa_data) > 0:\n",
        "            return qa_data[0].get(\"answer\")\n",
        "        else:\n",
        "            print(\"Unexpected qa_data structure:\", qa_data, \"with type\", type(qa_data))\n",
        "    except Exception as e:\n",
        "        print(\"Error parsing qa entry:\", qa_entry, \"\\nError:\", e)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61O0wFYbPKy9",
        "outputId": "7f6a8077-be5f-432c-865a-098dbc5abc46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                  qa  emotion\n",
            "0  [{'question': 'How does the person feel in the...      sad\n",
            "1  [{'question': 'How does the person feel in the...    anger\n",
            "2  [{'question': 'How does the person feel in the...  neutral\n",
            "3  [{'question': 'How does the person feel in the...     fear\n",
            "4  [{'question': 'How does the person feel in the...  content\n"
          ]
        }
      ],
      "source": [
        "# Assuming df is your DataFrame that includes the 'qa' column\n",
        "df[\"emotion\"] = df[\"qa\"].apply(extract_emotion)\n",
        "\n",
        "# Verify the new column\n",
        "print(df[[\"qa\", \"emotion\"]].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApSH2Dy9Ta9S",
        "outputId": "d8690b65-6353-4984-cc47-3a00b2bb629d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Unique emotion labels:\n",
            "['sad' 'anger' 'neutral' 'fear' 'content' 'happy' 'disgust' 'surprise']\n",
            "\n",
            "Distribution of emotion labels:\n",
            "emotion\n",
            "surprise    1238\n",
            "neutral     1225\n",
            "sad         1184\n",
            "fear        1181\n",
            "anger       1175\n",
            "disgust     1165\n",
            "content     1144\n",
            "happy       1088\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check unique values and distribution of facial emotion labels\n",
        "if 'emotion' in df.columns:\n",
        "    print(\"\\nUnique emotion labels:\")\n",
        "    print(df['emotion'].unique())\n",
        "\n",
        "    print(\"\\nDistribution of emotion labels:\")\n",
        "    print(df['emotion'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3IdvF_4gTfrk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example: Plot a histogram for a numeric column, adjust 'score' to the relevant column name\n",
        "if 'score' in df.columns:\n",
        "    plt.hist(df['score'].dropna(), bins=30, edgecolor='k')\n",
        "    plt.xlabel(\"Score\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(\"Histogram of Scores\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIh1nhSkTqb-",
        "outputId": "0500dfd4-0da1-4ae6-b295-21654994f554"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               image  emotion\n",
            "0  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...      sad\n",
            "1  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...    anger\n",
            "2  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  neutral\n",
            "3  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...     fear\n",
            "4  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  content\n"
          ]
        }
      ],
      "source": [
        "# Optionally, save the cleaned dataframe to disk as a new parquet file or CSV\n",
        "df.to_parquet(\"cleaned_human_face_emotions.parquet\")\n",
        "# Alternatively, you can save as CSV:\n",
        "# df.to_csv(\"cleaned_human_face_emotions.csv\", index=False)\n",
        "\n",
        "# drop qa column\n",
        "df.drop(columns=[\"qa\"], inplace=True)\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "verJk70Osn1p"
      },
      "source": [
        "Now we just have images in the first column with the emotion in the second column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S29bqfcisvkF",
        "outputId": "e82f13eb-d09f-4daf-e8b6-befaeb7bb3d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 5640\n",
            "Test set size: 1880\n",
            "Validation set size: 1880\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate feature (X) and label (y)\n",
        "X = df['image']\n",
        "y = df['emotion']\n",
        "\n",
        "# Perform a stratified split to keep class distribution consistent\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,      # 80% training, 20% testing\n",
        "    random_state=42,    # for reproducibility\n",
        "    stratify=y          # important for classification\n",
        ")\n",
        "\n",
        "# Validation split from X_train if needed:\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    test_size=0.25,     # 25% of the training set (which is 20% of the total) -> 15% overall\n",
        "    random_state=42,\n",
        "    stratify=y_train\n",
        ")\n",
        "\n",
        "print(\"Training set size:\", len(X_train))\n",
        "print(\"Test set size:\", len(X_test))\n",
        "print(\"Validation set size:\", len(X_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axs_6JBfvjIj",
        "outputId": "8907b8ca-db5c-426d-9477-5cdd0002077f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Decoding and resizing images...\n",
            "X_train_array shape: (5640, 224, 224, 3)\n",
            "X_val_array shape: (1880, 224, 224, 3)\n",
            "X_test_array shape: (1880, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "# Image bytes -> numpy arrays\n",
        "def decode_images(image_series, target_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Takes a pandas Series of dictionaries, each containing {'bytes': ...}.\n",
        "    Decodes them into a list of NumPy arrays (RGB).\n",
        "    Resizes images to target_size.\n",
        "    Normalizes pixel values to [0, 1].\n",
        "\n",
        "    Returns:\n",
        "      - A NumPy array of shape (num_samples, target_size[0], target_size[1], 3)\n",
        "    \"\"\"\n",
        "    decoded_list = []\n",
        "    for item in image_series:\n",
        "        # item should be a dict like {'bytes': b'...'}\n",
        "        try:\n",
        "            img_bytes = item['bytes']\n",
        "            with Image.open(io.BytesIO(img_bytes)) as img:\n",
        "                # Convert to RGB if needed\n",
        "                img = img.convert('RGB')\n",
        "                # Resize\n",
        "                img = img.resize(target_size)\n",
        "                # Convert to array\n",
        "                arr = np.array(img, dtype=np.float32) / 255.0\n",
        "            decoded_list.append(arr)\n",
        "        except Exception as e:\n",
        "            # If there's a bad image, you might want to handle or skip it\n",
        "            print(\"Error decoding image:\", e)\n",
        "            # Optionally skip or handle it somehow. For now, let's skip:\n",
        "            # Continue with the loop\n",
        "            continue\n",
        "\n",
        "    return np.stack(decoded_list, axis=0)\n",
        "\n",
        "print(\"\\nDecoding and resizing images...\")\n",
        "\n",
        "# Decode train set\n",
        "X_train_array = decode_images(X_train, target_size=(224, 224))\n",
        "print(\"X_train_array shape:\", X_train_array.shape)\n",
        "\n",
        "# Decode val set\n",
        "X_val_array = decode_images(X_val, target_size=(224, 224))\n",
        "print(\"X_val_array shape:\", X_val_array.shape)\n",
        "\n",
        "# Decode test set\n",
        "X_test_array = decode_images(X_test, target_size=(224, 224))\n",
        "print(\"X_test_array shape:\", X_test_array.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkTPeqPcxluw",
        "outputId": "f5ef3f62-9d4a-46a2-e4c8-a65a10b4b013"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Label classes found: ['anger' 'content' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'surprise']\n",
            "Sample of encoded labels: [6 3 5 6 0 6 7 3 4 0]\n"
          ]
        }
      ],
      "source": [
        "# Encode labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_val_encoded   = label_encoder.transform(y_val)\n",
        "y_test_encoded  = label_encoder.transform(y_test)\n",
        "\n",
        "print(\"\\nLabel classes found:\", label_encoder.classes_)\n",
        "print(\"Sample of encoded labels:\", y_train_encoded[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "cUbjnsEEyVTa",
        "outputId": "507bf7c9-5fce-40f6-d970-5c6dc2334141"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ sequential_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_3      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,248</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ sequential_6 (\u001b[38;5;33mSequential\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_3      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │        \u001b[38;5;34m10,248\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,268,232</span> (8.65 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,268,232\u001b[0m (8.65 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,248</span> (40.03 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,248\u001b[0m (40.03 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 117ms/step - accuracy: 0.1871 - loss: 2.1967 - val_accuracy: 0.2691 - val_loss: 1.8935\n",
            "Epoch 2/5\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 105ms/step - accuracy: 0.2599 - loss: 1.9593 - val_accuracy: 0.3181 - val_loss: 1.8175\n",
            "Epoch 3/5\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 100ms/step - accuracy: 0.2965 - loss: 1.8548 - val_accuracy: 0.3324 - val_loss: 1.7996\n",
            "Epoch 4/5\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 101ms/step - accuracy: 0.3362 - loss: 1.7889 - val_accuracy: 0.3319 - val_loss: 1.7817\n",
            "Epoch 5/5\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 102ms/step - accuracy: 0.3518 - loss: 1.7505 - val_accuracy: 0.3277 - val_loss: 1.7997\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 68ms/step - accuracy: 0.3215 - loss: 1.8659\n",
            "\n",
            "Test Loss: 1.8364\n",
            "Test Accuracy: 0.3303\n",
            "Epoch 1/5\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 484ms/step - accuracy: 0.2656 - loss: 2.0305 - val_accuracy: 0.2963 - val_loss: 1.9705\n",
            "Epoch 2/5\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 460ms/step - accuracy: 0.3104 - loss: 1.8336 - val_accuracy: 0.3186 - val_loss: 1.9613\n",
            "Epoch 3/5\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 452ms/step - accuracy: 0.3414 - loss: 1.7593 - val_accuracy: 0.3266 - val_loss: 1.9053\n",
            "Epoch 4/5\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 452ms/step - accuracy: 0.3548 - loss: 1.7056 - val_accuracy: 0.3383 - val_loss: 1.8805\n",
            "Epoch 5/5\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 462ms/step - accuracy: 0.3818 - loss: 1.6539 - val_accuracy: 0.3447 - val_loss: 1.8085\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - accuracy: 0.3324 - loss: 1.8867\n",
            "\n",
            "Final Test Loss after fine-tuning: 1.8291\n",
            "Final Test Accuracy after fine-tuning: 0.3559\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Suppose you have: (224, 224, 3) images\n",
        "# If you used a different size (e.g. 160x160 for MobileNet), be consistent\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# 1) Load a MobileNetV2 (or EfficientNet, ResNet, etc.) without its top layers\n",
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# 2) Freeze the base_model so we only train the new head first\n",
        "base_model.trainable = False\n",
        "\n",
        "# Define a simple data augmentation pipeline:\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    # Random rotation: factor=0.25 corresponds roughly to ±90° rotation.\n",
        "    layers.RandomRotation(0.25),\n",
        "    # Random horizontal flip.\n",
        "    layers.RandomFlip(\"horizontal\")\n",
        "])\n",
        "\n",
        "# 3) Build your classifier on top\n",
        "model = models.Sequential([\n",
        "    tf.keras.Input(shape=(224,224,3)),\n",
        "    # Data augmentation layers (only active during training).\n",
        "    data_augmentation,\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# 4) Train the new top layers\n",
        "history = model.fit(\n",
        "    X_train_array, y_train_encoded,\n",
        "    validation_data=(X_val_array, y_val_encoded),\n",
        "    epochs=5,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(X_test_array, y_test_encoded)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# 5) (Optional) Fine-tune deeper layers\n",
        "# Unfreeze part (or all) of base_model and re-compile with a lower learning rate\n",
        "base_model.trainable = True\n",
        "# You can selectively unfreeze only some layers:\n",
        "# for layer in base_model.layers[:100]:\n",
        "#     layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),  # smaller LR for fine-tuning\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_fine = model.fit(\n",
        "    X_train_array, y_train_encoded,\n",
        "    validation_data=(X_val_array, y_val_encoded),\n",
        "    epochs=5,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test_array, y_test_encoded)\n",
        "print(f\"\\nFinal Test Loss after fine-tuning: {test_loss:.4f}\")\n",
        "print(f\"Final Test Accuracy after fine-tuning: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model in the native TensorFlow SavedModel format.\n",
        "model.save(\"saved_MobileNetV2.keras\")\n",
        "\n",
        "# Load the model later.\n",
        "loaded_model = tf.keras.models.load_model(\"saved_MobileNetV2.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(\"saved_MobileNetV2.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "The layer sequential_7 has never been called and thus has no defined input.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[40], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# 3) Now do Grad-CAM:\u001b[39;00m\n\u001b[1;32m     53\u001b[0m test_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m \u001b[43mget_gradcam_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblock_16_project\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m superimposed \u001b[38;5;241m=\u001b[39m superimpose_heatmap(test_image, heatmap)\n\u001b[1;32m     56\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(cv2\u001b[38;5;241m.\u001b[39mcvtColor(superimposed, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB))\n",
            "Cell \u001b[0;32mIn[40], line 19\u001b[0m, in \u001b[0;36mget_gradcam_heatmap\u001b[0;34m(model, image, last_conv_layer_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m     base_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmobilenetv2_1.00_224\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     last_conv_layer \u001b[38;5;241m=\u001b[39m base_model\u001b[38;5;241m.\u001b[39mget_layer(last_conv_layer_name)\n\u001b[1;32m     18\u001b[0m grad_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel(\n\u001b[0;32m---> 19\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m,\n\u001b[1;32m     20\u001b[0m     outputs\u001b[38;5;241m=\u001b[39m[last_conv_layer\u001b[38;5;241m.\u001b[39moutput, model\u001b[38;5;241m.\u001b[39moutput]\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m image_batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tf-metal/lib/python3.9/site-packages/keras/src/ops/operation.py:276\u001b[0m, in \u001b[0;36mOperation.input\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the input tensor(s) of a symbolic operation.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    Only returns the tensor(s) corresponding to the *first time*\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m        Input tensor or list of input tensors.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_attribute_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_tensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tf-metal/lib/python3.9/site-packages/keras/src/ops/operation.py:307\u001b[0m, in \u001b[0;36mOperation._get_node_attribute_at_index\u001b[0;34m(self, node_index, attr, attr_name)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03mThis is used to implement the properties:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m    The operation's attribute `attr` at the node of index `node_index`.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has never been called \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand thus has no defined \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m     )\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes) \u001b[38;5;241m>\u001b[39m node_index:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at node \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but the operation has only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inbound nodes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    316\u001b[0m     )\n",
            "\u001b[0;31mAttributeError\u001b[0m: The layer sequential_7 has never been called and thus has no defined input."
          ]
        }
      ],
      "source": [
        "# [21] cell\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Load the PREVIOUSLY trained model from disk:\n",
        "model = tf.keras.models.load_model(\"saved_MobileNetV2.keras\")\n",
        "\n",
        "def get_gradcam_heatmap(model, image, last_conv_layer_name=\"block_16_project\"):\n",
        "    try:\n",
        "        last_conv_layer = model.get_layer(last_conv_layer_name)\n",
        "    except ValueError:\n",
        "        base_model = model.get_layer(\"mobilenetv2_1.00_224\")\n",
        "        last_conv_layer = base_model.get_layer(last_conv_layer_name)\n",
        "\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        inputs=model.input,\n",
        "        outputs=[last_conv_layer.output, model.output]\n",
        "    )\n",
        "    image_batch = np.expand_dims(image, axis=0)\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(image_batch)\n",
        "        pred_index = tf.argmax(predictions[0])\n",
        "        loss = predictions[:, pred_index]\n",
        "\n",
        "    grads = tape.gradient(loss, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))\n",
        "    \n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / (tf.reduce_max(heatmap)+1e-10)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "def superimpose_heatmap(original_img, heatmap, alpha=0.4):\n",
        "    if original_img.max() <= 1.0:\n",
        "        original_img = (original_img * 255).astype(\"uint8\")\n",
        "    else:\n",
        "        original_img = original_img.astype(\"uint8\")\n",
        "    heatmap_resized = cv2.resize(heatmap, \n",
        "                                 (original_img.shape[1], original_img.shape[0]))\n",
        "    heatmap_resized = np.uint8(255 * heatmap_resized)\n",
        "    jet = cv2.applyColorMap(heatmap_resized, cv2.COLORMAP_JET)\n",
        "    return cv2.addWeighted(jet, alpha, original_img, 1 - alpha, 0)\n",
        "\n",
        "# 2) IMPORTANT: call the model once:\n",
        "dummy_input = np.zeros((1,224,224,3), dtype=\"float32\")\n",
        "_ = model(dummy_input)\n",
        "\n",
        "# 3) Now do Grad-CAM:\n",
        "test_image = np.random.rand(224,224,3).astype(\"float32\")\n",
        "heatmap = get_gradcam_heatmap(model, test_image, \"block_16_project\")\n",
        "superimposed = superimpose_heatmap(test_image, heatmap)\n",
        "plt.imshow(cv2.cvtColor(superimposed, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf-metal",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
