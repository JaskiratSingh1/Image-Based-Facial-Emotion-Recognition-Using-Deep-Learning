{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Emotion Recognition using EfficientNetB0\n",
    "\n",
    "### Face Emotion Recognition (EfficientNetB0 & Fine-tuning Strategy)\n",
    "\n",
    "Dataset: https://huggingface.co/datasets/tukey/human_face_emotions_roboflow/\n",
    "\n",
    "\n",
    "## Goal\n",
    "This notebook trains a deep learning model to classify facial emotions from images using transfer learning with EfficientNetB0.\n",
    "\n",
    "## Dataset\n",
    "The data is sourced from the `tukey/human_face_emotions_roboflow` dataset available on the Hugging Face Hub. It contains images of faces labeled with one of several emotions (e.g., happy, sad, anger, etc.).\n",
    "\n",
    "## Model\n",
    "The model utilizes the EfficientNetB0 architecture, pre-trained on the ImageNet dataset. Transfer learning is applied by:\n",
    "1.  Replacing the original classification head with a new one suitable for the number of emotion classes in this dataset.\n",
    "2.  Initially training only the new head while keeping the EfficientNetB0 base frozen.\n",
    "3.  Fine-tuning the model by unfreezing the top layers of the EfficientNetB0 base and training the entire network with a very low learning rate.\n",
    "\n",
    "## Workflow\n",
    "1.  **Load Data:** Fetches the Parquet dataset file directly from the Hugging Face Hub.\n",
    "2.  **Clean Data:** Extracts emotion labels from the structured 'qa' column, converts labels to lowercase, and handles potential missing data.\n",
    "3.  **Split Data:** Divides the dataset into training (60%), validation (20%), and test (20%) sets using stratified splitting to maintain class distribution.\n",
    "4.  **Preprocess:**\n",
    "    * Decodes image bytes into NumPy arrays (uint8 format).\n",
    "    * Resizes images to the target size (224x224 for EfficientNetB0).\n",
    "    * Handles potential image decoding errors and filters corresponding labels.\n",
    "    * Encodes the string emotion labels into numerical format using `LabelEncoder`.\n",
    "5.  **Model Building:**\n",
    "    * Defines data augmentation layers (e.g., flips, rotations) to improve model generalization.\n",
    "    * Loads the pre-trained EfficientNetB0 base model (weights from ImageNet) without its top classification layer.\n",
    "    * Adds a global average pooling layer, a dropout layer, and a final dense layer (with softmax activation) for classification.\n",
    "    * Applies EfficientNet-specific preprocessing within the model.\n",
    "6.  **Training:**\n",
    "    * **Initial Phase:** Trains only the newly added classification head with the base model frozen. Uses `Adam` optimizer and `sparse_categorical_crossentropy` loss. Includes `EarlyStopping` based on validation loss.\n",
    "    * **Fine-tuning Phase:** Unfreezes the top ~30 layers of the EfficientNetB0 base model. Recompiles the model with a much lower learning rate and continues training. Also uses `EarlyStopping`.\n",
    "7.  **Evaluation:**\n",
    "    * Evaluates the final model's performance on the unseen test set (reporting loss and accuracy).\n",
    "    * Plots training and validation accuracy/loss curves over epochs.\n",
    "    * Generates a classification report (precision, recall, F1-score per class).\n",
    "    * Displays a confusion matrix heatmap.\n",
    "8.  **Save Model:** Saves the trained model to a `.keras` file.\n",
    "9.  **Qualitative Analysis:** Shows a few sample images from the test set with their true and predicted labels.\n",
    "\n",
    "## Requirements\n",
    "-   `pandas`\n",
    "-   `numpy`\n",
    "-   `tensorflow` (>= 2.x)\n",
    "-   `scikit-learn`\n",
    "-   `matplotlib`\n",
    "-   `seaborn`\n",
    "-   `Pillow` (PIL)\n",
    "-   `huggingface_hub` (or `datasets`) for loading data from the hub (`pip install huggingface_hub`)\n",
    "\n",
    "## Usage\n",
    "1.  Ensure all required libraries are installed.\n",
    "2.  Run the notebook cells sequentially.\n",
    "3.  **GPU Recommended:** Training deep learning models, especially fine-tuning, is computationally intensive. Running this notebook on a machine with a GPU (e.g., Google Colab, Kaggle, or a local setup) is highly recommended for reasonable training times.\n",
    "4.  Monitor the output, especially the training logs and evaluation plots, to understand model performance and convergence.\n",
    "\n",
    "## Key Parameters (Adjustable)\n",
    "-   `DATASET_PATH`: Path to the Parquet file on Hugging Face Hub.\n",
    "-   `TARGET_SIZE`: Input image dimensions (default 224x224).\n",
    "-   `BATCH_SIZE`: Number of samples per training step (default 32). Adjust based on GPU memory.\n",
    "-   `INITIAL_EPOCHS`: Maximum epochs for initial head training (default 15).\n",
    "-   `FINE_TUNE_EPOCHS`: Maximum additional epochs for fine-tuning (default 15).\n",
    "-   `fine_tune_layers_count`: Number of top layers to unfreeze in the base model during fine-tuning (default 30).\n",
    "-   `initial_learning_rate`: Learning rate for head training (default 0.001).\n",
    "-   `fine_tune_learning_rate`: Learning rate for fine-tuning (default 1e-5).\n",
    "-   `EarlyStopping` `patience`: Number of epochs to wait for improvement before stopping (default 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB2\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess_input\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Optional: Configure GPU memory growth if needed\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   try:\n",
    "#     for gpu in gpus:\n",
    "#       tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#   except RuntimeError as e:\n",
    "#     print(e)\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load Data from Hugging Face Hub\n",
    "DATASET_PATH = \"hf://datasets/tukey/human_face_emotions_roboflow/data/train-00000-of-00001.parquet\"\n",
    "try:\n",
    "    df = pd.read_parquet(DATASET_PATH)\n",
    "    print(f\"Data loaded successfully from {DATASET_PATH}.\")\n",
    "    print(f\"Initial shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    # Handle error, e.g., exit or try loading locally\n",
    "    df = pd.DataFrame() # Assign empty dataframe to avoid further errors if loading fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Chunk 3: Clean Data and Extract Labels\n",
    "if not df.empty:\n",
    "    # Standardize column names\n",
    "    df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]\n",
    "\n",
    "    # Function to extract emotion label\n",
    "    def extract_emotion(qa_entry):\n",
    "        try:\n",
    "            if isinstance(qa_entry, str):\n",
    "                qa_entry = qa_entry.strip()\n",
    "                qa_data = json.loads(qa_entry)\n",
    "            else:\n",
    "                qa_data = qa_entry\n",
    "\n",
    "            if isinstance(qa_data, np.ndarray):\n",
    "                qa_data = qa_data.tolist()\n",
    "\n",
    "            if isinstance(qa_data, (list, tuple)) and len(qa_data) > 0:\n",
    "                answer = qa_data[0].get(\"answer\")\n",
    "                if isinstance(answer, str):\n",
    "                    return answer.lower().strip() # Standardize labels\n",
    "                else:\n",
    "                    # print(f\"Unexpected answer type: {answer}, type: {type(answer)} in entry: {qa_entry}\")\n",
    "                    return None\n",
    "            else:\n",
    "                # print(f\"Unexpected qa_data structure: {qa_data}, type: {type(qa_data)}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            # print(f\"Error parsing qa entry: {qa_entry}, \\nError: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Apply the function\n",
    "    df[\"emotion\"] = df[\"qa\"].apply(extract_emotion)\n",
    "\n",
    "    # Check and handle missing values\n",
    "    print(\"\\nMissing values BEFORE handling:\")\n",
    "    print(df.isna().sum())\n",
    "    initial_rows = len(df)\n",
    "    df.dropna(subset=['image', 'emotion'], inplace=True) # Drop if image or extracted emotion is missing\n",
    "    print(f\"Dropped {initial_rows - len(df)} rows due to missing image or emotion labels.\")\n",
    "    print(\"\\nMissing values AFTER handling:\")\n",
    "    print(df.isna().sum())\n",
    "\n",
    "\n",
    "    # Drop the original 'qa' column\n",
    "    if 'qa' in df.columns:\n",
    "        df.drop(columns=[\"qa\"], inplace=True)\n",
    "\n",
    "    # Display info and head\n",
    "    print(\"\\nCleaned Dataframe Info:\")\n",
    "    df.info()\n",
    "    print(\"\\nCleaned Dataframe Head:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Check unique values and distribution\n",
    "    print(\"\\nUnique emotion labels:\")\n",
    "    unique_labels = df['emotion'].unique()\n",
    "    print(unique_labels)\n",
    "    num_classes = len(unique_labels)\n",
    "    print(f\"\\nNumber of unique labels: {num_classes}\")\n",
    "\n",
    "    print(\"\\nDistribution of emotion labels:\")\n",
    "    print(df['emotion'].value_counts())\n",
    "else:\n",
    "    print(\"DataFrame is empty, skipping cleaning steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    X = df['image']\n",
    "    y = df['emotion']\n",
    "\n",
    "    # Stratified split into Train (60%), Validation (20%), Test (20%)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.4,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=0.5,\n",
    "        random_state=42,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "\n",
    "    print(f\"\\nData Split:\")\n",
    "    print(f\"Training set size: {len(X_train)}\")\n",
    "    print(f\"Validation set size: {len(X_val)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "else:\n",
    "    print(\"DataFrame is empty, skipping data splitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Preprocessing & Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Image Decoding Function\n",
    "def decode_images(image_series, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Decodes image bytes from a pandas Series into NumPy arrays.\n",
    "    Handles potential errors during decoding.\n",
    "    Returns decoded images (uint8) and indices of valid images.\n",
    "    \"\"\"\n",
    "    decoded_list = []\n",
    "    valid_indices = []\n",
    "    original_indices = image_series.index\n",
    "\n",
    "    print(f\"Attempting to decode {len(image_series)} images...\")\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    for i, item in enumerate(image_series):\n",
    "        try:\n",
    "            img_bytes = item['bytes']\n",
    "            with Image.open(io.BytesIO(img_bytes)) as img:\n",
    "                img = img.convert('RGB') # Ensure 3 channels\n",
    "                img = img.resize(target_size, Image.Resampling.LANCZOS) # Use LANCZOS for quality\n",
    "                # Keep as uint8 for EfficientNet preprocessing\n",
    "                arr = np.array(img, dtype=np.uint8)\n",
    "            decoded_list.append(arr)\n",
    "            valid_indices.append(original_indices[i])\n",
    "            processed_count += 1\n",
    "        except Exception as e:\n",
    "            # print(f\"Error decoding image at index {original_indices[i]}: {e}. Skipping.\")\n",
    "            error_count += 1\n",
    "            continue\n",
    "        # Optional: Print progress periodically\n",
    "        # if (i + 1) % 500 == 0:\n",
    "        #     print(f\"  Processed {i+1}/{len(image_series)} images...\")\n",
    "\n",
    "    print(f\"Successfully decoded: {processed_count}, Errors: {error_count}\")\n",
    "\n",
    "    if not decoded_list:\n",
    "        return np.array([]), []\n",
    "\n",
    "    return np.stack(decoded_list, axis=0), valid_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Decoding and Filter Labels\n",
    "if 'X_train' in locals(): # Check if splitting was done\n",
    "    TARGET_SIZE = (224, 224) # Default for EfficientNetB0\n",
    "    print(\"\\nDecoding Training Images...\")\n",
    "    X_train_array, train_valid_indices = decode_images(X_train, target_size=TARGET_SIZE)\n",
    "    print(\"\\nDecoding Validation Images...\")\n",
    "    X_val_array, val_valid_indices = decode_images(X_val, target_size=TARGET_SIZE)\n",
    "    print(\"\\nDecoding Test Images...\")\n",
    "    X_test_array, test_valid_indices = decode_images(X_test, target_size=TARGET_SIZE)\n",
    "\n",
    "    print(f\"\\nX_train_array shape: {X_train_array.shape if X_train_array.size > 0 else 'Empty'}\")\n",
    "    print(f\"X_val_array shape: {X_val_array.shape if X_val_array.size > 0 else 'Empty'}\")\n",
    "    print(f\"X_test_array shape: {X_test_array.shape if X_test_array.size > 0 else 'Empty'}\")\n",
    "\n",
    "    # Filter labels corresponding to successfully decoded images\n",
    "    y_train = y_train.loc[train_valid_indices]\n",
    "    y_val = y_val.loc[val_valid_indices]\n",
    "    y_test = y_test.loc[test_valid_indices]\n",
    "\n",
    "    print(f\"\\nFiltered label counts:\")\n",
    "    print(f\"y_train: {len(y_train)}\")\n",
    "    print(f\"y_val: {len(y_val)}\")\n",
    "    print(f\"y_test: {len(y_test)}\")\n",
    "\n",
    "    # Check if any set became empty after filtering bad images\n",
    "    if X_train_array.size == 0 or X_val_array.size == 0 or X_test_array.size == 0:\n",
    "        raise ValueError(\"One or more data splits empty after image decoding. Check data source/errors.\")\n",
    "else:\n",
    "    print(\"Skipping image decoding as data splits not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Encode Labels\n",
    "if 'y_train' in locals():\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_val_encoded   = label_encoder.transform(y_val)\n",
    "    y_test_encoded  = label_encoder.transform(y_test)\n",
    "\n",
    "    # Recalculate num_classes based on fitted encoder\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    print(\"\\nLabel classes found:\", label_encoder.classes_)\n",
    "    print(\"Number of classes:\", num_classes)\n",
    "    print(\"Sample of encoded labels (train):\", y_train_encoded[:10])\n",
    "else:\n",
    "    print(\"Skipping label encoding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Building (EfficientNetB0 with Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Chunk 8: Define Data Augmentation and Load Base Model\n",
    "IMG_SHAPE = TARGET_SIZE + (3,)\n",
    "\n",
    "# Data Augmentation Layer\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.1),\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "# Load EfficientNetB0 base model\n",
    "try:\n",
    "    base_model = EfficientNetB2(\n",
    "        input_shape=IMG_SHAPE,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    # Freeze the base model initially\n",
    "    base_model.trainable = False\n",
    "    print(\"EfficientNetB0 base model loaded and frozen.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading base model: {e}\")\n",
    "    base_model = None # Set to None to prevent errors later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Build Full Model\n",
    "if base_model is not None and 'num_classes' in locals():\n",
    "    inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "    x = data_augmentation(inputs)\n",
    "    # Use EfficientNetB0 preprocessing (expects uint8 0-255)\n",
    "    x = efficientnet_preprocess_input(x)\n",
    "    x = base_model(x, training=False) # Run base in inference mode initially\n",
    "    x = layers.GlobalAveragePooling2D(name=\"global_avg_pool\")(x)\n",
    "    x = layers.Dropout(0.2, name=\"top_dropout\")(x) # Regularization dropout\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name=\"output_dense\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model for initial training\n",
    "    initial_learning_rate = 0.001\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(\"\\nModel built successfully.\")\n",
    "    model.summary()\n",
    "else:\n",
    "    print(\"Skipping model building due to previous errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initial Training (Train the Head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initial Model Training\n",
    "if 'model' in locals() and X_train_array.size > 0:\n",
    "    INITIAL_EPOCHS = 50 # Adjust as needed\n",
    "    BATCH_SIZE = 32     # Adjust based on GPU memory\n",
    "\n",
    "    # EarlyStopping Callback\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5, # Stop if val_loss doesn't improve for 5 epochs\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\nStarting initial training for up to {INITIAL_EPOCHS} epochs...\")\n",
    "    history = model.fit(\n",
    "        X_train_array, y_train_encoded,\n",
    "        epochs=INITIAL_EPOCHS,\n",
    "        validation_data=(X_val_array, y_val_encoded),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Evaluate after initial training\n",
    "    loss0, accuracy0 = model.evaluate(X_val_array, y_val_encoded, verbose=0)\n",
    "    print(f\"\\nInitial training complete (or stopped early).\")\n",
    "    print(f\"Initial training - Validation Loss: {loss0:.4f}\")\n",
    "    print(f\"Initial training - Validation Accuracy: {accuracy0:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping initial training due to previous errors or empty data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fine-tuning (Adjusted Strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare for Fine-tuning (Unfreeze Layers)\n",
    "if 'model' in locals() and 'history' in locals():\n",
    "    # Unfreeze the base model first\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Determine how many layers are in the base model\n",
    "    total_layers = len(base_model.layers)\n",
    "    print(f\"\\nTotal layers in base model: {total_layers}\")\n",
    "\n",
    "    # Decide layers to fine-tune (e.g., unfreeze top 30)\n",
    "    fine_tune_layers_count = 30\n",
    "    fine_tune_from_layer_index = total_layers - fine_tune_layers_count\n",
    "    print(f\"Fine-tuning the top {fine_tune_layers_count} layers (from index {fine_tune_from_layer_index} onwards).\")\n",
    "\n",
    "    # Freeze all layers before the `fine_tune_from_layer_index`\n",
    "    for layer in base_model.layers[:fine_tune_from_layer_index]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Re-compile with a very low learning rate\n",
    "    fine_tune_learning_rate = 1e-5\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_learning_rate),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print(\"\\nModel Summary after setting fine-tuning layers:\")\n",
    "    model.summary()\n",
    "else:\n",
    "    print(\"\\nSkipping fine-tuning setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Run Fine-tuning Training\n",
    "if 'model' in locals() and 'history' in locals() and base_model.trainable: # Check if fine-tuning was set up\n",
    "    FINE_TUNE_EPOCHS = 35 # Adjust as needed\n",
    "    start_epoch_ft = history.epoch[-1] + 1 if history.epoch else 0\n",
    "    total_epochs_target = start_epoch_ft + FINE_TUNE_EPOCHS\n",
    "\n",
    "    print(f\"\\nStarting fine-tuning from epoch {start_epoch_ft} up to {total_epochs_target-1}...\")\n",
    "\n",
    "    # Use a new EarlyStopping instance\n",
    "    early_stopping_ft = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Check input data type again before fitting\n",
    "    print(f\"Fine-tuning input data type: {X_train_array.dtype}\") # Should be uint8\n",
    "\n",
    "    history_fine = model.fit(\n",
    "        X_train_array, y_train_encoded,\n",
    "        epochs=total_epochs_target,\n",
    "        initial_epoch=start_epoch_ft,\n",
    "        validation_data=(X_val_array, y_val_encoded),\n",
    "        batch_size=BATCH_SIZE, # Consider reducing batch size if memory issues arise\n",
    "        callbacks=[early_stopping_ft]\n",
    "    )\n",
    "    print(\"\\nFine-tuning complete (or stopped early).\")\n",
    "else:\n",
    "    print(\"\\nSkipping fine-tuning training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Final Evaluation on Test Set\n",
    "if 'model' in locals() and X_test_array.size > 0:\n",
    "    print(\"\\nEvaluating final model on Test Set...\")\n",
    "    test_loss, test_acc = model.evaluate(X_test_array, y_test_encoded)\n",
    "    print(f\"\\nFinal Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "else:\n",
    "    print(\"\\nSkipping final evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot Training History\n",
    "if 'history' in locals():\n",
    "    # Combine histories safely\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_initial = len(acc)\n",
    "\n",
    "    fine_tune_epochs_run = 0\n",
    "    if 'history_fine' in locals() and history_fine.history:\n",
    "        acc += history_fine.history.get('accuracy', [])\n",
    "        val_acc += history_fine.history.get('val_accuracy', [])\n",
    "        loss += history_fine.history.get('loss', [])\n",
    "        val_loss += history_fine.history.get('val_loss', [])\n",
    "        fine_tune_epochs_run = len(history_fine.history.get('loss', []))\n",
    "\n",
    "    epochs_range = range(epochs_initial + fine_tune_epochs_run)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.axvline(x=epochs_initial -1 , color='r', linestyle='--', label='Start Fine-tuning') # Mark fine-tuning start\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.axvline(x=epochs_initial -1, color='r', linestyle='--', label='Start Fine-tuning') # Mark fine-tuning start\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nSkipping history plotting as training did not run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Classification Report and Confusion Matrix\n",
    "if 'model' in locals() and X_test_array.size > 0 and 'y_test_encoded' in locals():\n",
    "    print(\"\\nGenerating predictions on test set...\")\n",
    "    y_pred_probs = model.predict(X_test_array)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_encoded, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test_encoded, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=label_encoder.classes_,\n",
    "                yticklabels=label_encoder.classes_)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix (EfficientNetB0)\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nSkipping Classification Report and Confusion Matrix generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save the Trained Model\n",
    "if 'model' in locals():\n",
    "    model_save_path = \"face_emotion_efficientnetb0_final.keras\"\n",
    "    try:\n",
    "        model.save(model_save_path)\n",
    "        print(f\"Model saved successfully to {model_save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping model saving.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Qualitative Analysis (Sample Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Display Sample Predictions\n",
    "if 'model' in locals() and X_test_array.size > 0 and 'y_test_encoded' in locals():\n",
    "    num_samples = 5\n",
    "    if len(X_test_array) >= num_samples:\n",
    "        indices = np.random.choice(np.arange(len(X_test_array)), num_samples, replace=False)\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, idx in enumerate(indices):\n",
    "            plt.subplot(1, num_samples, i+1)\n",
    "            # EfficientNet input was uint8 [0, 255]\n",
    "            img_display = X_test_array[idx]\n",
    "            plt.imshow(img_display)\n",
    "            true_label = label_encoder.classes_[y_test_encoded[idx]]\n",
    "            pred_label = label_encoder.classes_[y_pred[idx]]\n",
    "            plt.title(f\"True: {true_label}\\nPred: {pred_label}\")\n",
    "            plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nNot enough test samples to display.\")\n",
    "else:\n",
    "    print(\"\\nSkipping qualitative analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#model.summary()\n",
    "#effnet_model = model.get_layer('efficientnetb2')\n",
    "#effnet_model.summary()\n",
    "#model.summary(expand_nested=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
